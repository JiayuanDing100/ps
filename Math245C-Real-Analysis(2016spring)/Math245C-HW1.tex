\documentclass[12pt,a4paper]{article}
	%[fleqn] %%% --to make all equation left-algned--

\usepackage[top=1.2in, bottom=1.2in, left=.7in, right=.7in]{geometry}
%\usepackage{fullpage}

\usepackage{fancyhdr}\pagestyle{fancy}\rhead{Stephanie Wang}\lhead{Math245C - Homework 1}

\usepackage{amsmath,amssymb,amsthm,amsfonts,microtype,stmaryrd}
%{mathtools,wasysym,yhmath}

\usepackage[usenames,dvipsnames]{xcolor}\newcommand{\blue}[1]{\textcolor{blue}{#1}}\newcommand{\red}[1]{\textcolor{red}{#1}}\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\fgreen}[1]{\textcolor{ForestGreen}{#1}}

\usepackage{mdframed}
	%\newtheorem{mdexample}{Example}
	%\definecolor{warmgreen}{rgb}{0.8,0.9,0.85}
	% --Example:
	% \begin{center}
	% \begin{minipage}{0.8,0.9,0.85\textwidth}
	% \begin{mdframed}[backgroundcolor=warmgreen, 
	% skipabove=4pt,skipbelow=4pt,hidealllines=true, 
	% topline=false,leftline=false,middlelinewidth=10pt, 
	% roundcorner=10pt] 
	%%%% --CONTENTS-- %%%%
	% \end{mdframed}\end{minipage}\end{center}	

%\usepackage{graphicx}
%\graphicspath{ {/Users/KoraJr/Documents/MATLAB} }
	% --Example:
	% \includegraphics[scale=0.5]{picture name}
%\usepackage{caption} %%% --some awful package to make caption...

%\usepackage{hyperref}\hypersetup{linktocpage,colorlinks}\hypersetup{citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

%%% --Text Fonts
%\usepackage{times} %%% --Times New Roman for LaTeX
%\usepackage{fontspec}\setmainfont{Times New Roman} %%% --Times New Roman; XeLaTeX only

%%% --Math Fonts
%\renewcommand{\mbf}[1]{\mathbf{#1}} %%% --vector
%\newcommand{\ca}[1]{\mathcal{#1}} %%% --"bigO"
%\newcommand{\bb}[1]{\mathbb{#1}} %%% --"Natural, Real numbers"
%\newcommand{\rom}[1]{\romannumeral{#1}} %%% --Roman numbers

%%% --Quick Arrows
\newcommand{\ra}[1]{\ifnum #1=1\rightarrow\fi\ifnum #1=2\Rightarrow\fi}

\newcommand{\la}[1]{\ifnum #1=1 \leftarrow\fi}

%%% --Special Editor Config
\renewcommand{\ni}{\noindent}
\newcommand{\onum}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-1pt} {#1}}}}
\newcommand{\bbu}{\blacktriangleright}
\newcommand{\wbu}{\vartriangleright}

\newcommand{\claim}[1]{\underline{``{#1}":}}
\newcommand{\prob}[1]{\bf {#1}}

\newcommand{\bgfl}{\begin{flalign*}}
\newcommand{\bga}{\begin{align*}}
\def\beq{\begin{equation}} \def\eeq{\end{equation}}

\renewcommand{\l}{\left}\renewcommand{\r}{\right}

\newcommand{\casebrak}[2]{\left \{ \begin{array}{l} {#1}\\{#2} \end{array} \right.}
%\newcommand{\ttm}[4]{\l[\begin{array}{cc}{#1}&{#2}\\{#3}&{#4}\end{array}\r]} %two-by-two-matrix
%\newcommand{\tv}[2]{\l[\begin{array}{c}{#1}\\{#2}\end{array}\r]}

\newcommand{\dps}{\displaystyle}

\let\italiccorrection=\/
\def\/{\ifmmode\expandafter\frac\else\italiccorrection\fi}


%%% --General Math Symbols
\newcommand{\bc}{\because}
\newcommand{\tf}{\therefore}
\newcommand{\SUM}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\PROD}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\CUP}[2]{\bigcup\limits_{#1}^{#2}}
\newcommand{\CAP}[2]{\bigcap\limits_{#1}^{#2}}
\newcommand{\SUP}[1]{\sup\limits_{#1}}

\renewcommand{\o}{\circ}
\newcommand{\x}{\times}
\newcommand{\ox}{\otimes}

%%% --Special Math Characters
\newcommand{\R}{\mathbb R}%Real number
\newcommand{\N}{\mathbb N}%Nature number
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\A}{\mathcal{A}}%measurable sets
\renewcommand{\P}{\mathcal{P}}%power set

%%% --REAL ANALYSIS Symbols
\newcommand{\INT}[2]{\int_{#1}^{#2}}
\newcommand{\pdiff}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\UPINT}{\bar\int}
\newcommand{\UPINTRd}{\overline{\int_{\bb R ^d}}}
\newcommand{\supp}{\text{supp}}

\newcommand{\leb}{\lambda^\ast} %%% --Lebesgue
\renewcommand{\H}[1]{{\cal H}^{#1}} %%% --Hausdorff
\newcommand{\B}{\mathcal{B}} %%% --Borel set
\newcommand{\cL}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}} %%% --index set
\newcommand{\Supp}[1]{\text{Supp}\left({#1}\right)}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\subsection*{Problem 1 (Exercise 1.7.6. [Tao, 2011])}
Relying on Theorem 1.7.12, since $L$ is surjective, there exists a constant $C >0$ such that 
$$\forall y \in Y \exists x\in X, Lx = y \wedge \|x\|_X \leq C\|y\|_Y$$
Now fix $\lambda \in Y^\ast$, 
\bga
\|L^\ast \lambda\|_{X^\ast}
& = \sup_{x\in X\setminus \{0\}}\/{|\lambda(Lx)|}{\|x\|_X} 
\end{align*}
Now consider the supremum taken over the smaller set
$$\l\{\/{|\lambda(y)|}{\|x\|_X}: y \in Y\setminus\{0\}, Lx = y, \|x\|_X \leq C \|y\|_Y\r\}$$
This is indeed a smaller subset since $L$ is surjective and zero is only mapped to zero. This leads to our goal
\bga
\|L^\ast \lambda\|_{X^\ast}
& \geq \sup \l\{\/{|\lambda(y)|}{\|x\|_X}: y \in Y\setminus \{0\}, Lx = y, \|x\|_X \leq C \|y\|_Y\r\}\\
& \geq \sup \l\{\/{|\lambda(y)|}{C\|y\|_X}: y \in Y\setminus\{0\}, Lx = y, \|x\|_X \leq C \|y\|_Y\r\} \\
& = \sup\l\{\/{|\lambda(y)|}{C\|y\|_X}: y \in Y\setminus\{0\}\r\} = \/1C\|\lambda\|_{Y^\ast}
\end{align*}
Let $c = \/1C$ then we're done with the first part. It's rather easy to show $L^\ast$ is also bounded from above, fix $\lambda \in Y^\ast$, 
\bga
\|L^\ast \lambda\|_{X^\ast}
& = \sup_{x\in X\setminus \{0\}}\/{|\lambda(Lx)|}{\|x\|_X} \\
& \leq \sup_{x\in X\setminus \{0\}}\/{\|\lambda\|_{Y^\ast}\|Lx\|_Y}{\|x\|_X} \\
& = \sup_{x\in X\setminus \{0\}}\/{\|Lx\|_Y}{\|x\|_X} \cdot \|\lambda\|_{Y^\ast}\\
& = \|L\|_{op}\|\lambda\|_{Y^\ast}
\end{align*}
So $\|L^\ast\|_{op} \leq \|L\|_{op}$. \qed


\newpage\subsection*{Problem 2 (Exercise 1.7.7. [Tao, 2011])}
For each $y\in H'$, take (from surjectivity of $L$) $x \in H$ such that $Lx = y$. Since $H = ker(L) \oplus ker(L)^\perp$, $x = v + z$ where $v \in ker(L)$ and $z\in ker(L)^\perp$. Define $Sy := z$. This way the range of $S$ falls naturally in $ker(L)^\perp$. \\
\claim{$S$ is well-defined} Suppose both $v_1, v_2 \in ker(L)^\perp$ such that $Lv_1 = Lv_2 = y$, then $L(v_1 - v_2) = 0$ and $v_1 - v_2 \in ker(L)$, and $v_1 = v_2$.\\
\claim{$S$: linear transformation} For $y_1, y_2 \in H'$, $\alpha, \beta \in \mathbb F$, take $v_1, v_2 \in ker(L)^\perp$ such that $L(v_1) = y_1, L(v_2) = y_2$. Then certainly (since $L$: linear)
$$L(\alpha v_1 + \beta v_2) = \alpha y_1 + \beta y_2$$
and $\alpha v_1 + \beta v_2 \in ker(L)^\perp$ (since $ker(L)^\perp$ is a subspace). \\
\claim{$S$: bounded} Since $L$: surjective, by Open Mapping Theorem there exists constant $C>0$ such that 
$$\forall y\in H' \exists x\in H, Lx = y \wedge \|x\|_H \leq C\|y\|_{H'}$$
Therefore from how we defined $S$, we see 
$$C\|y\|_{H'} \geq \|x\|_H \geq \|v\|_H + \|z\|_H \geq \|Sy\|_H$$
and $\|S\|_{op} \leq C$ is bounded. \\
\claim{$LS = I$} By our construction of $S$, $L(S y) = Lz = L(v+z) = y$. \qed

\newpage\subsection*{Problem 3 (Exercise 1.7.8. [Tao, 2011])} 
Since the other direction is beyond words... \claim{(i) $\ra2$ (ii)}\\
First observe algebraically speaking (i) asserts $X = M \oplus N$. Now define two linear transformation (projection)
$$\begin{array}{ccccc}
\pi_M: & X \ra1 M & \text{ and }& \pi_N: & X \ra1 N\\
& x\mapsto m &&& x \mapsto n
\end{array}$$
according to the unique decomposition $x = m + n$. With knowledge from linear algebra, both $\pi_M$ and $\pi_N$ are linear, and all we need to show is they are bounded. \fgreen{It's too hard to prove it directly, so let's apply the MAGIC-----} Consider the inverse map of $\pi_M \x \pi_N$
$$\begin{array}{rl}
\varphi: M \x N &\ra1 X\\
		(m, n) &\mapsto m+n
\end{array}$$
Here equip $M\x N$ a norm $\|(m, n)\|_{M\x N} := \|m\|_X + \|n\|_X$ then it's true this makes $M\x N$ a Banach space. Furthermore, this map is bounded since 
$$\|\varphi(m, n)\|_X = \|m+n\|_X \leq \|m\|_X + \|n\|_X = \|(m, n)\|_{M\x N}$$
the inequality follows from simply triangle inequality. Also, this is a surjective map since every vector $x \in X$ has a decomposition $x = m+n$. By open mapping theorem, we see $\varphi$ is open. Therefore the inverse $\pi_M \x \pi_N$ and its components $\pi_M, \pi_N$ are naturally bounded. \fgreen{Booh!} \qed





\newpage\subsection*{Problem 4 (Exercise 1.7.10. [Tao, 2011])}
\claim{Case \onum1: finite dimensional subspace} Let $X$ be the Banach space and $V = span(v_1, \cdots, v_n)$ be a finite-dimensional subspace of $X$ and $v_1, \cdots, v_n$ are linearly independent. For each $x\in V$, we have the unique representation
$$x = \alpha_1 v_1 + \cdots + \alpha_n v_n$$
Consider these linear functionals $\alpha_1, \cdots, \alpha_n \in (V \ra1 \mathbb F)$, they're bounded by $\|v_1\|_X, \cdots, \|v_n\|_X$; therefore, by applying Hahn Banach theorem we can extend them into $\overline{\alpha_1}, \cdots, \overline{\alpha_n} \in X^\ast$. Define the map $P: X \ra1 V$ by
$$P(x) := \overline{\alpha_1}(x) v_1 + \cdots + \overline{\alpha_n}(x) v_n \text{ for }x \in X$$
Then we can see $P \mid V \equiv I_V$ hence $P^2 = P$ and clearly for each $x\in X$, 
$$x = P(x) + (x - P(x))$$
where $P(x) \in V$ and $(x - P(x)) \in ker(P)$. Here $ker(P) \cap V = \{0\}$ also holds due to $P\mid V \equiv I_V$. We see that $X = V \oplus ker(P)$. \\
\\
\claim{Case \onum2: finite co-dimensional subspace} With the same settings except $X/V = span(f_1 + V, \cdots, f_n+V)$ where $f_1, \cdots, f_n \in X$ were chosen to be the basis and may vary by a vector in $V$. For each $x\in X$, we have a representation 
$$x = \alpha_1 f_1 + \cdots + \alpha_n f_n + v \text{ for some } v\in V$$
where the coefficients $\alpha_1, \cdots, \alpha_n$ corresponds to the coordinate of $x + V$ in $X/V$ are unique. Consider these linear functional $\alpha_1, \cdots, \alpha_n \in X^\ast$, define $P: X \ra1 span(f_1, \cdots, f_n)$ by
$$P(x) := \alpha_1(x) f_1 + \cdots + \alpha_n(x) f_n \text{ for } x\in X$$
Then we can see $P \mid span(f_1, \cdots, f_n) \equiv I_{span(f_1, \cdots, f_n)}$ hence $P^2 = P$ and clearly for each $x\in X$, 
$$x = P(x) + (x-P(x))$$
where $P(x) \in span(f_1, \cdots, f_n)$ and $(x - P(x)) \in ker(P)$. Here $ker(P) \cap span(f_1, \cdots, f_n) = \{0\}$ holds too and $X = ker(P) \oplus span(f_1, \cdots, f_n)$. It remains to show that $ker(P) = V$. It's true that $V \subseteq ker(P)$ since for each $v\in V$, $v + V \sim 0 + V$. On the other hand, for $x \notin V$, $x + V \nsim 0+V$, therefore $\alpha_1(x) f_1 + \cdots + \alpha_n(x) f_n \neq 0$, ie. $P(x) \neq 0$. \qed

\newpage\subsection*{Problem 5 (Exercise 1.4.18. [Tao, 2011])}
(i) First note that orthonormality makes the connection between the finite sums
$$\SUM{n=1}N |c_n|^2 = \l\|\SUM{n=1}N c_ne_n\r\|^2$$
Loosely speaking we can conclude that the convergence of one side can lead to that of the other side. However, to be completely rigorous, we need to rely on Cauchy sequence notion. \\
\claim{if} Since $c_n$'s are square summable, for $M, N \in \N$, $M>N$, 
\bga
\l\|\SUM{n=N}M c_n e_n \r\|^2 & = \l\langle \SUM{n=N}M c_n e_n , \SUM{n=N}M c_n e_n \r\rangle\\
& = \SUM{n=N}M |c_n|^2 \ra1 0 \text{ as } M, N \ra1\infty
\end{align*}
Therefore $\l\{\SUM{n=1}N c_ne_n\r\}_{N\in\N}$ is a Cauchy sequence in $H$ and henceforth convergent. \\
\claim{and only if} Since norm is continuous, if we denote the limit $l := \SUM{n=1}\infty c_ne_n$, then
\bga
\text{(as } \infty  \leftarrow N\text{, ) }  \|l\|^2 & \leftarrow \l\|\SUM{n=1}N c_ne_n\r\|^2 \\
& = \SUM{n=1}N |c_n|^2
\end{align*}
Hence the convergence of the scalar square sum is ensured. Also from continuity of norm, we see 
$$\l\|\SUM{n=1}\infty c_n e_n\r\|^2 = \SUM{n=1}\infty |c_n|^2$$
(ii) Fix a permutation $\sigma: \N \ra1 \N$. For $\epsilon >0$, take $N_0\in\N$ such that
$$\forall N \geq N_0, \SUM{n=N}\infty |c_n|^2 < \epsilon$$
Now take $N_1 \in \N$ be such that 
$$\{1, \cdots, N_0\} \subseteq \sigma\l(\{1, \cdots, N_1\}\r)$$
(That is, we exclude all the possible $n$'s that makes huge portion in the sum of $\|c_n\|^2$'s. ) Now for $M, N \geq N_1$, 
\bga
\l\|\SUM{n=N}{M} c_{\sigma(n)} e_{\sigma(n)}\r\|
& \leq \SUM{n=N}{M} |c_{\sigma(n)}|^2 \\
& \leq \SUM{n=N_0}\infty |c_n|^2 < \epsilon
\end{align*}
Therefore the permuted sum $\l\{\SUM{n=1}N c_{\sigma(n)} e_{\sigma(n)}\r\}_{N\in\N}$ still forms a Cauchy sequence and henceforth converges in the Hilbert space. \\
\\
(iii) Denote the map by $L$. \\
\claim{$L$: well-defined} From part (i) we know $(c_n)_{n=1}^\infty \in \ell^2(\N)$ assures the image $\SUM{n=1}\infty c_n e_n$ to be a valid vector in $H$. \\
\\
\claim{$L$: linear transform} Consider $\mathbf b = (b_n)_{n=1}^\infty, \mathbf c = (c_n)_{n=1}^\infty \in \ell^2(\N)$ and $\alpha, \beta \in \mathbb F$, then 
\bga
L(\alpha \mathbf b + \beta \mathbf c)
& = \SUM{n=1}\infty (\alpha b_n + \beta c_n)e_n \\
\alpha L(\mathbf b) + \beta L(\mathbf c) & = \alpha\SUM{n=1}\infty b_ne_n + \beta\SUM{n=1}\infty c_ne_n
\end{align*}
For arbitrary $\epsilon > 0$, take (from the convergence of these vector sums) $N_1, N_2, N_3 \in \N$ such that 
$$\begin{array}{c}
\forall N \geq N_1, \l\|\SUM{n=1}N b_ne_n - \SUM{n=1}\infty b_ne_n \r\| < \epsilon \\
\forall N \geq N_2, \l\|\SUM{n=1}N c_ne_n - \SUM{n=1}\infty c_ne_n \r\| < \epsilon \\ 
\forall N \geq N_3, \l\|\SUM{n=1}N (\alpha b_ne_n + \beta c_n e_n) - \SUM{n=1}\infty (\alpha b_ne_n + \beta c_ne_n) \r\| < \epsilon 
\end{array}$$
Then for $N \geq \max\{N_1, N_2, N_3\}$, 
\bga
\l\|\alpha L(\mathbf b) + \beta L(\mathbf c) - L(\alpha \mathbf b + \beta \mathbf c)\r\|
\leq&  \l\|\alpha L(\mathbf b) + \beta L(\mathbf c) - \SUM{n=1}N \alpha b_n + \beta c_n\r\| \\
& + \l\|\SUM{n=1}N \alpha b_n + \beta c_n - L(\alpha \mathbf b + \beta \mathbf c)\r\| \\
\leq & |\alpha|\epsilon + |\beta|\epsilon + \epsilon \ra1 0 \text{ as } \epsilon \ra1 0
\end{align*}
\claim{$L$: bijective} Given a convergent series of vectors, we can find the corresponding sequence of numbers that is square summable (from part (i)). This gives surjectivity. 

To show injectivity, suppose for contradiction that for some$(c_n)_{n=1}^\infty \in \ell^2(\N)$ such that 
$$\SUM{n=1}\infty c_n e_n = 0$$
or, equivalently, 
$$\l\|\SUM{n=1}\infty c_n e_n \r\| = 0$$
From the very last part of (i) we see this leads to 
$$\SUM{n=1}\infty |c_n|^2 = 0$$
However this is a nonnegative sum and we easily conclude that $(c_n)_{n=1}^\infty = (0)_{n=1}^\infty$. \\
\\
\claim{$L$: isometry} This comes directly as the last part of (i). \\
\\
\claim{$V = Im(L)$ is the smallest closed subspace of $H$ that contains $e_1,e_2,\cdots$} Certainly for all $i \in \N, (\delta_{in})_{n=1}^\infty \in \ell^2(\N)$, therefore $e_i \in V$. Also for $W \subseteq X$ be a closed subspace containing $e_1, e_2 \cdots$, $W$ would contain 
$$\SUM{n=1}N c_n e_n \text{ for any } N\in\N, c_n \in \mathbb F$$
Therefore by taking the limit, if the sum converges (which is ensure by the square summability of $c_n$'s), the limit $\SUM{n=1}\infty c_n e_n \in W$ since $W$ is closed. \\
\\
(iv) Consider the adjoint $L^\ast: H^\ast \ra1 \ell^2(\N)^\ast$. By Riesz representation theorem on Hilbert spaces, we know $H^\ast \cong H$ and $\ell^2(\N)^\ast \cong \ell^2(\N)$ (here $\cong$ denotes isometrically equivalent). In particular, for $x\in H$, 
$$L^\ast(\langle \cdot, x \rangle) = \langle \cdot, (c_m)_{m=1}^\infty \rangle_{\ell^2(\N)}$$
and for $n\in\N$, $L^\ast(\langle \cdot, e_n \rangle) = \langle \cdot, \delta_{mn}\rangle_{\ell^2(\N)}$, this suggests (by linearity)
$$L^\ast(\langle \cdot, x \rangle) = \l\langle \cdot, \SUM{n=1}\infty c_n \delta_{mn}\r\rangle_{\ell^2(\N)} = L^\ast\l(\l\langle \cdot, \SUM{n=1}\infty c_n e_n\r\rangle\r)$$
And relying on $L^\ast$ being an isometry, we see 
$$\forall n\in\N, c_n = \l\langle \SUM{m=1}\infty c_m, \delta_{mn}\r\rangle_{\ell^2(\N)} = \langle x, e_n\rangle$$
Define $\pi_V = L^\ast$ under some isomorphism, that is, 
$$\pi_V(x) = \SUM{n=1}\infty c_n e_n = \SUM{n=1}\infty \langle x, e_n \rangle e_n$$
Then we get the Bessel's inequality (by applying results from part (i))
$$\|x\| \geq \|\pi_V(x)\|^2 = \l\|\SUM{n=1}\infty \langle x, e_n\rangle e_n\r\|^2 = \SUM{n=1}\infty |\langle x, e_n\rangle|^2$$
\qed\\
\fgreen{I feel my arguments needs to be improved here, but I'm really sick and my head hurts now. I will come back and revise it later. But I believe the whole idea is $\pi_V$ is the composition of maps going $H \ra1 H^\ast \ra1 \ell^2(\N)^\ast \ra1 \ell^2(\N) \ra1 H$ where on each stage the map is isometry (between the domain and the image)...}
\newpage\subsection*{Problem 6 (Folland 5.3.30(b))}
{\it Problem \; }If $Y = C([0,1])$ and $X = C^1([0,1])$ are equipped with the uniform norm, show that the derivative operator $d/dx$ from $X$ to $Y$ is closed but not bounded. \\
\\
{\it Answer \; } \claim{$d/dx$: closed} To show the graph
$$\{(f, f'): f \in C^1([0, 1])\} \subseteq C^1([0, 1]) \x C([0, 1])$$
is closed, let us take a sequence $\{f_n\}_{n=1}^\infty \subseteq C^1([0, 1])$ such that 
$$\|f_n - f\|_u \ra1 0 \text{ and } \|f_n' - g\|_u \ra1 0$$ for some functions $f\in C^1([0, 1])$ and $g \in C([0, 1])$. However it's well known that uniformly convergent differentiable functions will have their derivatives converges to the derivative of their limit (if exists). This proves the limit of $(f_n, f_n')$ is $(f, g) = (f, f')$, also in the set of the graph.\\
\\
\claim{$d/dx$: unbounded} Consider $\{f_n\}_{n\in\N} \subseteq C^1([0, 1])$ with
$$f_n(x) = x^n$$
then fairly $\forall n \in \N, \|f_n\|_u = 1$ yet $f_n'(x) = nx^{n-1}$ and $\|f_n\|_u = n \ra1 \infty$ for $n\ra1\infty$. Conclude that $d/dx$ is unbounded. \qed
\end{document}